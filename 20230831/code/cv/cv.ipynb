{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=P size=559x536>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    " \n",
    "img = Image.open('PennFudanPed/PNGImages/FudanPed00001.png')\n",
    "mask = Image.open('PennFudanPed/PedMasks/FudanPed00001_mask.png')\n",
    "mask = mask.convert(\"P\")\n",
    "mask.putpalette([\n",
    "    0, 0, 0, # black background\n",
    "    255, 0, 0, # index 1 is red\n",
    "    255, 255, 0, # index 2 is yellow\n",
    "    255, 153, 0, # index 3 is orange\n",
    "])\n",
    " \n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PennFudanDataset('PennFudanPed/')\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "import torchvision.transforms as Tr\n",
    "def get_transform(train):\n",
    "    transform = []\n",
    "    transform.append([Tr.ToTensor()])\n",
    "    if train:\n",
    "        transform.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "\n",
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = 2\n",
    "    # use our dataset and defined transformations\n",
    "    dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "    dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, num_workers=0,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    print(\"That's it!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yezzz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\yezzz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\GitHub\\24-Machine-Learning-ZTF\\20230824\\code\\cv.ipynb 单元格 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     main()\n",
      "\u001b[1;32md:\\Documents\\GitHub\\24-Machine-Learning-ZTF\\20230824\\code\\cv.ipynb 单元格 7\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m# update the learning rate\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\24-Machine-Learning-ZTF\\20230824\\code\\engine.py:27\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[0;32m     21\u001b[0m     warmup_iters \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39m1000\u001b[39m, \u001b[39mlen\u001b[39m(data_loader) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m     lr_scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mLinearLR(\n\u001b[0;32m     24\u001b[0m         optimizer, start_factor\u001b[39m=\u001b[39mwarmup_factor, total_iters\u001b[39m=\u001b[39mwarmup_iters\n\u001b[0;32m     25\u001b[0m     )\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfor\u001b[39;00m images, targets \u001b[39min\u001b[39;00m metric_logger\u001b[39m.\u001b[39mlog_every(data_loader, print_freq, header):\n\u001b[0;32m     28\u001b[0m     images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[0;32m     29\u001b[0m     targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\24-Machine-Learning-ZTF\\20230824\\code\\utils.py:171\u001b[0m, in \u001b[0;36mMetricLogger.log_every\u001b[1;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[0;32m    167\u001b[0m     log_msg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelimiter\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m    168\u001b[0m         [header, \u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m space_fmt \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}/\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39meta: \u001b[39m\u001b[39m{eta}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m{meters}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtime: \u001b[39m\u001b[39m{time}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata: \u001b[39m\u001b[39m{data}\u001b[39;00m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    169\u001b[0m     )\n\u001b[0;32m    170\u001b[0m MB \u001b[39m=\u001b[39m \u001b[39m1024.0\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024.0\u001b[39m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m    172\u001b[0m     data_time\u001b[39m.\u001b[39mupdate(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m end)\n\u001b[0;32m    173\u001b[0m     \u001b[39myield\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\yezzz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\yezzz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\yezzz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\yezzz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\yezzz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;32md:\\Documents\\GitHub\\24-Machine-Learning-ZTF\\20230824\\code\\cv.ipynb 单元格 7\u001b[0m in \u001b[0;36mPennFudanDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m target[\u001b[39m\"\u001b[39m\u001b[39miscrowd\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m iscrowd\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     img, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms(img, target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/24-Machine-Learning-ZTF/20230824/code/cv.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mreturn\u001b[39;00m img, target\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\24-Machine-Learning-ZTF\\20230824\\code\\transforms.py:26\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, image, target)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, image, target):\n\u001b[0;32m     25\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 26\u001b[0m         image, target \u001b[39m=\u001b[39m t(image, target)\n\u001b[0;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m image, target\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
