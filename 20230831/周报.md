### 1. 学习内容

+ Pytorch 还在学第五部分 "Image and Video"
+ 学习并编写了 MLP, FFNN, RBFNN
+ 将几种优化算法代码实现

### 2. 碰到的一些问题

+ 学习pytorch的过程中经常会碰到一些版本不对应的问题（前几天一个报错查了半天，才发现先是版本的问题）

  解决办法：要学会使用虚拟环境（之前就一股脑全放在主环境中）

## （Ps. 图片显示还有点问题，明天会补上）

### 3. 几种优化算法

**一. 实验内容**

**在二维参数空间，实现下列算法，并比较它们对参数的影响。**

**关于参数的函数为：$ f = 0.1*w_1^2 + 2*w_2^2 $ **

1 Root Mean Square Propagation 算法实现

2 Adaptive Delta Propagation 算法实现



3 SGD with momentum 算法实现



4 Adam 算法实现



通过上述算法，将初始点(-5,-2)不断地更新至（0,0）。

 

**二. 实验分析**

标准的梯度下降法中，每个参数在每次迭代时都使用相同的学习率。

AdaGrad算法是借鉴 ℓ2 正则化的思想，每次迭代时自适应地调整每个参数的学习率。在第𝑡次迭代时，先计算每个参数梯度平方的累计值。在AdaGrad算法中，如果某个参数的偏导数累积比较大，学习率相对较小；相反，如果其偏导数累积较小，其学习率相对较大。整体是随着迭代次数的增加，学习率逐渐缩小。AaGrad 算法的缺点是在经过一定次数的迭代依然没有找到最优点时，由 于这时的学习率已经非常小，很难再继续找到最优点。

RMSprop算法是一种自适应学习率的方法，可以在有些情况下避免 AdaGrad 算法中学习率不断单调下降以至于过早衰减的缺点。和 AdaGrad 算法的区别在于 𝐺𝑡 的计算由累积方式变成了指数衰减移动平均。

AdaDelta算法也是 AdaGrad 算法的一个改进。和 RMSprop 算法类似，AdaDelta 算法通过梯度平方的指数衰减移动平均来调整学习率。

动量法（Momentum Method）是用之前积累动量来替代真正的梯度，每次迭代的梯度可以看作加速度。

Adam算法可以看作动量法和 RMSprop 算法的结合，不但使用动量作为参数更新方向，而 且可以自适应调整学习率。

**三. 实验结果**



整体实验效果如上图所示（迭代轮数为50轮）



梯度下降算法（lr = 0.2, rounds = 60），初始下降快，越接近零点，导函数越小，下降越慢。



AdaGrad（lr = 0.8（初始）， rounds = 25），以每个参数梯度的平方的累计值开根号作为学习率的分母，w1梯度平方累计值较大，学习率偏小；w2梯度平方累计值较小，学习率偏大，但由于w2对梯度下降的影响更大，故迭代轮数较gd算法偏少。



RMSprop算法（lr=0.6（初始学习率）， beta=0.9（衰减率）， rounds=8），在AdaGrad基础上引入衰减率，使得w2的学习率一直比较高（beta=0.9的情况下），故能在8轮下降到零点。



AdaDelta算法（beta=0.9， rounds=15），也能较快下降至零点，但由于学习率是梯度的平方，带入参数，下降过程中会下降，因此与rmsprop算法相比，下降轮数更多。



动量法（lr=0.05, roh=0.9（动量因子）， rounds=25），因为是每次参数的更新用梯度的累计去更新，下降较快，但容易振荡。



Adam算法（rounds=25），结合了动量法和rmsprop，下降较快。
